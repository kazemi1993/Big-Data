# In[1]:


from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('hack_find').getOrCreate()


# In[2]:


from pyspark.ml.clustering import KMeans

# Loads data.
dataset = spark.read.csv("hack_data.csv",header=True,inferSchema=True)


# In[3]:


dataset.head()


# In[4]:


dataset.describe().show()


# In[5]:


dataset.columns


# In[6]:


from pyspark.ml.linalg import Vectors
from pyspark.ml.feature import VectorAssembler


# In[7]:


feat_cols = ['Session_Connection_Time', 'Bytes Transferred', 'Kali_Trace_Used',
             'Servers_Corrupted', 'Pages_Corrupted','WPM_Typing_Speed']


# In[8]:


vec_assembler = VectorAssembler(inputCols = feat_cols, outputCol='features')


# In[9]:


final_data = vec_assembler.transform(dataset)


# In[10]:


from pyspark.ml.feature import StandardScaler


# In[11]:


scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures")


# In[12]:


# Compute summary statistics by fitting the StandardScaler
scalerModel = scaler.fit(final_data)


# In[13]:


# Normalize each feature to have unit standard deviation.
cluster_final_data = scalerModel.transform(final_data)


# In[14]:


cluster_final_data.printSchema()


# In[15]:


cluster_final_data.show()


# ** Time to find out whether its 2 or 3! **

# In[16]:


#default inputs
# kmeans = KMeans(featuresCol='features', predictionCol='prediction')


# In[17]:


# Time to find out whether its 2 or 3!
kmeans3 = KMeans(featuresCol='scaledFeatures',k=3)
kmeans2 = KMeans(featuresCol='scaledFeatures',k=2)


# In[18]:


model_k3 = kmeans3.fit(cluster_final_data)
model_k2 = kmeans2.fit(cluster_final_data)


# In[19]:


model_k2.transform(cluster_final_data).groupBy('prediction').count().show()


# In[20]:


model_k3.transform(cluster_final_data).groupBy('prediction').count().show()

