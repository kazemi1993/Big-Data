import findspark
findspark.init("C:\spark\spark-3.0.1-bin-hadoop2.7")
findspark.find()
import pyspark

# In[1]:


import findspark
findspark.init("C:\spark\spark-3.1.2-bin-hadoop3.2")
findspark.find()
import pyspark


# In[2]:


from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("word count").setMaster("local[*]")
sc = SparkContext(conf = conf)

lines = sc.textFile("wordcount.text")

words = lines.flatMap(lambda line: line.split(" "))


wordCounts = words.countByValue()

for word, count in wordCounts.items():
    print("{} : {}".format(word, count))

